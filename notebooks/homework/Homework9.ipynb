{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Statistics for Physicists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "import torch.optim\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you will extend some of the studies described in the [Best Practices](https://nbviewer.jupyter.org/github/dkirkby/MachineLearningStatistics/blob/master/notebooks/NNTricks.ipynb) notebook. We start by recreating the training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "95781e65392ed9d47882a649b1b9bb07",
     "grade": false,
     "grade_id": "cell-6ddfb8edca9774dd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "x_train = 6 * torch.rand((1000, 1)) - 3\n",
    "y_train = torch.sin(x_train)\n",
    "x_test = 6 * torch.rand((100, 1)) - 3\n",
    "y_test = torch.sin(x_test)\n",
    "plt.plot(x_test.numpy(), y_test.numpy(), '.', label='TEST')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the tensors into Dataset objects to simplify passing them around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ea226b3a1dcf9620986e65bb99a7b067",
     "grade": false,
     "grade_id": "cell-27beb7be13ec8618",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "xy_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "xy_test = torch.utils.data.TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create (and save) the baseline model to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8ef0e068f0958f05717626334659f523",
     "grade": false,
     "grade_id": "cell-894ecfb188d78039",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "baseline = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 20),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20, 25),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(25, 1),\n",
    ")\n",
    "torch.save(baseline.state_dict(), 'baseline.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function below to build a train and test loop using hyperparameters specified as function arguments. Your code should produce a single scatter plot showing the TRAIN and TEST loss curves, similar to the ones we produced earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9cbc881c5d76edc861ea9fcb96352eb7",
     "grade": false,
     "grade_id": "cell-11235479a624a1eb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def learn(train_data, test_data, model, loss_fn=torch.nn.MSELoss(), n_epochs=200,\n",
    "          batch_size=200, learning_rate=0.1, momentum=0.9):\n",
    "    \"\"\"Perform a train and test loop with specified hyperparameters.\n",
    "    \n",
    "    Uses SGD optimization and produces a scatter plot of TRAIN and TEST\n",
    "    loss versus epoch on a log-linear scale.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : torch.utils.data.Dataset\n",
    "        Container for the training input and target tensors to use.\n",
    "    test_data : torch.utils.data.Dataset\n",
    "        Container for the test input and target tensors to use.\n",
    "    model : torch.nn.Module\n",
    "        Neural network model of the data whose parameters will be learned.\n",
    "    loss_fn : callable\n",
    "        Function of (y_out, y_tgt) that calculates the scalar loss to use.\n",
    "        Must support backwards() method.\n",
    "    n_epochs : int\n",
    "        Number of epochs of training to perform.\n",
    "    batch_size : int\n",
    "        Size of each (randomly shuffled) minibatch to use.\n",
    "    learning_rate : float\n",
    "        Learning rate to use for the SGD optimizer.\n",
    "    momentum : float\n",
    "        Momentum to use for the SGD optimizer.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple (train, test) of arrays of loss values after each epoch for\n",
    "        the TRAIN and TEST samples, respectively. Both lists should have\n",
    "        length equal to n_epochs.    \n",
    "    \"\"\"\n",
    "    losses_train, losses_test = [], []\n",
    "    loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    x_test, y_test = test_data.tensors\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    plt.plot(losses_train, '.', label='TRAIN')\n",
    "    plt.plot(losses_test, '.', label='TEST')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log');\n",
    "    return losses_train, losses_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code with the default hyperparameters using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "296c0095530ea4591a7b53a565bffe65",
     "grade": true,
     "grade_id": "cell-1d0c5e1abdea086a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "torch.manual_seed(123)\n",
    "baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "train, test = learn(xy_train, xy_test, baseline)\n",
    "assert train[0] > 0.1 and test[0] > 0.1\n",
    "assert train[15] < 1e-3 and test[15] < 1e-3\n",
    "assert train[-1] < 1e-4 and test[-1] < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to establish the same initial state (seed and model parameters) and repeat this learning loop with the optimizer momentum set to zero, to show its effect on the loss curves: large synchronized oscillations in both the TRAIN and TEST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b28e3d3c0f0d9fce30bc5ccc7f2adf2d",
     "grade": false,
     "grade_id": "cell-33341682dcd504f4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "learn(xy_train, xy_test, baseline, momentum=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will explore some of the \"hyperdecisions\" involved in selecting a network architecture, re-using your `learn()` function from the previous problem.\n",
    "\n",
    "First, vary the number of hidden nodes on the second layer from 25 to 10 (using the default `learn` hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4b9876ca02d4d43e6b3f4a26c56271dc",
     "grade": true,
     "grade_id": "cell-71368a8f24d58268",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, change both activations from `ReLU` to `Tanh` (going back to 25 nodes on the second layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "13765ac90011144c66ca245210d01289",
     "grade": true,
     "grade_id": "cell-85c6f55b38bdf3a3",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, go back to `ReLU` for the hidden activations, but add a `Tanh` activation on the output node, since it enforces an output in the range [-1,+1] which matches our target range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "70bb88adc3bb51babd400806fff3b817",
     "grade": true,
     "grade_id": "cell-678bb2f01d634c2a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will explore the effects of varying the learning rate according to some schedule, using the [torch.optim.lr_scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) utilities.\n",
    "\n",
    "Implement the function below to build a train and test loop that uses a specified learning rate schedule. You should be able to copy most of your code from your earlier `learn` function. Note that we need to create an optimizer before we can create a scheduler, so this changes the function's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fa14b29b4129948fc593e5e6047ee2ce",
     "grade": false,
     "grade_id": "cell-a320ae250b9916d5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def learn_with_schedule(train_data, test_data, model, scheduler,\n",
    "                        loss_fn=torch.nn.MSELoss(), n_epochs=200, batch_size=200):\n",
    "    \"\"\"Perform a train and test loop with specified hyperparameters.\n",
    "    \n",
    "    Uses SGD optimization with a learning rate schedule and produces a\n",
    "    scatter plot of TRAIN and TEST loss versus epoch on a log-linear scale.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : torch.utils.data.Dataset\n",
    "        Container for the training input and target tensors to use.\n",
    "    test_data : torch.utils.data.Dataset\n",
    "        Container for the test input and target tensors to use.\n",
    "    model : torch.nn.Module\n",
    "        Neural network model of the data whose parameters will be learned.\n",
    "    scheduler : object\n",
    "        Object with a step() method to update the learning rate schedule\n",
    "        before each training epoch.\n",
    "    loss_fn : callable\n",
    "        Function of (y_out, y_tgt) that calculates the scalar loss to use.\n",
    "        Must support backwards() method.\n",
    "    n_epochs : int\n",
    "        Number of epochs of training to perform.\n",
    "    batch_size : int\n",
    "        Size of each (randomly shuffled) minibatch to use.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple (train, test) of arrays of loss values after each epoch for\n",
    "        the TRAIN and TEST samples, respectively. Both lists should have\n",
    "        length equal to n_epochs.    \n",
    "    \"\"\"\n",
    "    losses_train, losses_test = [], []\n",
    "    loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    x_test, y_test = test_data.tensors\n",
    "    optimizer = scheduler.optimizer\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    plt.plot(losses_train, '.', label='TRAIN')\n",
    "    plt.plot(losses_test, '.', label='TEST')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log');\n",
    "    return losses_train, losses_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "efb80373052f09423d5de986f32a8890",
     "grade": true,
     "grade_id": "cell-0afcc34483c71347",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "optimizer = torch.optim.SGD(baseline.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# A constant scheduler should give identical results to the initial learn function.\n",
    "ConstantLR = lambda epoch: 1.\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, ConstantLR)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "train1, test1 = learn_with_schedule(xy_train, xy_test, baseline, scheduler)\n",
    "plt.show()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "train2, test2 = learn(xy_train, xy_test, baseline, learning_rate=0.1, momentum=0.9)\n",
    "plt.show()\n",
    "\n",
    "assert np.all(train1 == train2) and np.all(test1 == test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a suite of schedules on the same problem using your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7a9f2011061333f5b31de35874e5fa06",
     "grade": false,
     "grade_id": "cell-0477b72492952b1f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(baseline.parameters(), lr=0.1, momentum=0.9)\n",
    "torch.save(optimizer.state_dict(), 'optimizer.pth')\n",
    "\n",
    "def OneCyclePolicy(epoch, period=50, frac=0.1):\n",
    "    phase = (epoch % period) / period\n",
    "    gamma = frac ** (-2 / period)\n",
    "    #print(phase, gamma)\n",
    "    if phase < 0.5:\n",
    "        return frac * gamma ** (period * phase)\n",
    "    else:\n",
    "        return frac * gamma ** (period * (1 - phase))\n",
    "\n",
    "schedulers = {\n",
    "    'MultiStep': torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100, 150], gamma=0.5),\n",
    "    'Exponential': torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99),\n",
    "    'CosineAnnealing': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25, eta_min=0.01),\n",
    "    '1CyclePolicy': torch.optim.lr_scheduler.LambdaLR(optimizer, OneCyclePolicy),\n",
    "}\n",
    "\n",
    "for name, scheduler in schedulers.items():\n",
    "    torch.manual_seed(123)\n",
    "    baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "    optimizer.load_state_dict(torch.load('optimizer.pth'))\n",
    "    learn_with_schedule(xy_train, xy_test, baseline, scheduler)\n",
    "    plt.title(name, fontsize=14)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
